{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1: Dictionary-based Tokenization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you are to implement a dictionary-based word segmentation algorithm. There are two Python functions that you need to complete: \n",
    "<br>\n",
    "* maximal_matching\n",
    "* backtrack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a toy dictionary to test the algorithm\n",
    "\n",
    "This is based on the example shown in the lecture. \n",
    "You will tokenize the following text string: \"ไปหามเหสี!\"\n",
    "The toy dictoionary provided in this exercise includes all the charaters, syllables, and words that appear that the text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "thai_vocab = [\"ไ\",\"ป\",\"ห\",\"า\",\"ม\",\"เ\",\"ห\",\"ส\",\"ี\",\"ไป\",\"หา\",\"หาม\",\"เห\",\"สี\",\"มเหสี\",\"!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximal matching \n",
    "Complete the maximal matching  function below to tokenize the input text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf #infinity\n",
    "def maximal_matching(c):\n",
    "    #Initialize an empty 2D list\n",
    "    d  =[[None]*len(c) for _ in range(len(c))]\n",
    "    \n",
    "    ####FILL CODE HERE####\n",
    "    for i in range(len(c)):\n",
    "        for j in range(i, len(c), 1):\n",
    "            if i == 0 and c[i:j+1] in thai_vocab:\n",
    "                d[i][j] = 1\n",
    "            elif c[i:j+1] in thai_vocab:\n",
    "                d[i][j] = 1 + (min([m[i-1] for m in d[0:i]]))\n",
    "            else:\n",
    "                d[i][j] = inf\n",
    "    ######################\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtracking\n",
    "Complete the backtracking function below to find the tokenzied words.\n",
    "It should return a list containing a pair of the beginning position and the ending position of each word.\n",
    "In this example, it should return: \n",
    "<br>\n",
    "[(0, 1),(2, 3),(4, 8),(9, 9)]\n",
    "<br> \n",
    "#### Each pair contains the position of each word as follows:\n",
    "(0, 1) ไป\n",
    "<br>\n",
    "(2, 3) หา\n",
    "<br>\n",
    "(4, 8) มเหสี\n",
    "<br>\n",
    "(9, 9) !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack(d):\n",
    "    eow = len(d)-1 # End of Word position\n",
    "    word_pos = [] # Word position\n",
    "    ####FILL CODE HERE####\n",
    "    sow = inf\n",
    "    while sow != 0:\n",
    "        col_list = [x[eow] for x in d[0:eow+1]]\n",
    "        min_val = min(col_list)\n",
    "        sow = col_list.index(min_val)\n",
    "        word_pos.append((sow, eow))\n",
    "        eow = sow - 1\n",
    "    ######################\n",
    "    word_pos.reverse()\n",
    "    return word_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your maximal matching algorithm on a toy dictionary\n",
    "\n",
    "Expected output:\n",
    "\n",
    "[1, 1, inf, inf, inf, inf, inf, inf, inf, inf] ไ\n",
    "<br>\n",
    "[None, 2, inf, inf, inf, inf, inf, inf, inf, inf] ป\n",
    "<br>\n",
    "[None, None, 2, 2, 2, inf, inf, inf, inf, inf] ห\n",
    "<br>\n",
    "[None, None, None, 3, inf, inf, inf, inf, inf, inf] า\n",
    "<br>\n",
    "[None, None, None, None, 3, inf, inf, inf, 3, inf] ม\n",
    "<br>\n",
    "[None, None, None, None, None, 3, 3, inf, inf, inf] เ\n",
    "<br>\n",
    "[None, None, None, None, None, None, 4, inf, inf, inf] ห\n",
    "<br>\n",
    "[None, None, None, None, None, None, None, 4, 4, inf] ส\n",
    "<br>\n",
    "[None, None, None, None, None, None, None, None, 5, inf] ี\n",
    "<br>\n",
    "[None, None, None, None, None, None, None, None, None, 4] !\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, inf, inf, inf, inf, inf, inf, inf, inf] ไ\n",
      "[None, 2, inf, inf, inf, inf, inf, inf, inf, inf] ป\n",
      "[None, None, 2, 2, 2, inf, inf, inf, inf, inf] ห\n",
      "[None, None, None, 3, inf, inf, inf, inf, inf, inf] า\n",
      "[None, None, None, None, 3, inf, inf, inf, 3, inf] ม\n",
      "[None, None, None, None, None, 3, 3, inf, inf, inf] เ\n",
      "[None, None, None, None, None, None, 4, inf, inf, inf] ห\n",
      "[None, None, None, None, None, None, None, 4, 4, inf] ส\n",
      "[None, None, None, None, None, None, None, None, 5, inf] ี\n",
      "[None, None, None, None, None, None, None, None, None, 4] !\n"
     ]
    }
   ],
   "source": [
    "input_text = \"ไปหามเหสี!\"\n",
    "out = maximal_matching(input_text)\n",
    "for i in range(len(out)):\n",
    "    print(out[i],input_text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your backtracking algorithm on a toy dictionary\n",
    "Expected output:\n",
    "<br>\n",
    "ไป|หา|มเหสี|!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ไป|หา|มเหสี|!\n"
     ]
    }
   ],
   "source": [
    "def print_tokenized_text(d, input_text):\n",
    "    tokenized_text=[]\n",
    "    for pos in backtrack(d):\n",
    "        #print(pos)\n",
    "        tokenized_text.append(input_text[pos[0]:pos[1]+1])\n",
    "\n",
    "    print(\"|\".join(tokenized_text))\n",
    "    \n",
    "print_tokenized_text(out,input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now try it on a real dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For UNIX-based OS users, the following cell will download a dictionary (it's just a list of thai words). Alternatively, you can download it from this link: https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2019-01-13 12:17:47--  https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.8.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.8.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1560612 (1.5M) [text/plain]\n",
      "Saving to: 'words_th.txt.1'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  3%  477K 3s\n",
      "    50K .......... .......... .......... .......... ..........  6% 1.54M 2s\n",
      "   100K .......... .......... .......... .......... ..........  9% 5.59M 1s\n",
      "   150K .......... .......... .......... .......... .......... 13% 1.19M 1s\n",
      "   200K .......... .......... .......... .......... .......... 16% 1.19M 1s\n",
      "   250K .......... .......... .......... .......... .......... 19% 1.89M 1s\n",
      "   300K .......... .......... .......... .......... .......... 22% 7.94M 1s\n",
      "   350K .......... .......... .......... .......... .......... 26% 2.89M 1s\n",
      "   400K .......... .......... .......... .......... .......... 29% 7.29M 1s\n",
      "   450K .......... .......... .......... .......... .......... 32% 4.67M 1s\n",
      "   500K .......... .......... .......... .......... .......... 36% 8.45M 1s\n",
      "   550K .......... .......... .......... .......... .......... 39% 7.17M 0s\n",
      "   600K .......... .......... .......... .......... .......... 42% 5.86M 0s\n",
      "   650K .......... .......... .......... .......... .......... 45% 3.34M 0s\n",
      "   700K .......... .......... .......... .......... .......... 49% 6.89M 0s\n",
      "   750K .......... .......... .......... .......... .......... 52% 7.35M 0s\n",
      "   800K .......... .......... .......... .......... .......... 55% 8.40M 0s\n",
      "   850K .......... .......... .......... .......... .......... 59% 9.11M 0s\n",
      "   900K .......... .......... .......... .......... .......... 62% 8.25M 0s\n",
      "   950K .......... .......... .......... .......... .......... 65% 4.60M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 68% 7.87M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 72% 6.17M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 75% 7.58M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 78% 6.59M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 82% 7.97M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 85% 8.06M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 88% 5.19M 0s\n",
      "  1350K .......... .......... .......... .......... .......... 91% 7.03M 0s\n",
      "  1400K .......... .......... .......... .......... .......... 95% 6.84M 0s\n",
      "  1450K .......... .......... .......... .......... .......... 98%  117K 0s\n",
      "  1500K .......... .......... ....                            100%  606K=0.9s\n",
      "\n",
      "2019-01-13 12:17:49 (1.65 MB/s) - 'words_th.txt.1' saved [1560612/1560612]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 63529\n",
      "['ก.', 'ก.ค.', 'ก.ต.', 'ก.ป.ส.', 'ก.พ.', 'ก.พ.ด.', 'ก.ม.', 'ก.ย', 'ก.ย.', 'ก.ร.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"words_th.txt\",encoding='utf-8-sig') as f:\n",
    "    thai_vocab = f.read().splitlines() \n",
    "print(\"Vocab size:\", len(thai_vocab))\n",
    "print(thai_vocab[:10])\n",
    "#you can add more vocab to the dictionary \n",
    "thai_vocab.extend([\"ๆ\",\"!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The output of your maximal matching algorithm on a new dictionary\n",
    "Expected output:\n",
    "<br>\n",
    "[1, 1, inf, 1, inf, inf, inf, inf, inf] ไ\n",
    "<br>\n",
    "[None, 2, inf, inf, inf, inf, inf, inf, inf] ป\n",
    "<br>\n",
    "[None, None, 2, 2, 2, inf, inf, inf, inf] ห\n",
    "<br>\n",
    "[None, None, None, inf, inf, inf, inf, inf, inf] า\n",
    "<br>\n",
    "[None, None, None, None, 2, inf, inf, inf, 2] ม\n",
    "<br>\n",
    "[None, None, None, None, None, inf, 3, inf, inf] เ\n",
    "<br>\n",
    "[None, None, None, None, None, None, inf, inf, inf] ห\n",
    "<br>\n",
    "[None, None, None, None, None, None, None, 4, 4] ส\n",
    "<br>\n",
    "[None, None, None, None, None, None, None, None, inf] ี"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inf, 1, inf, 1, inf, inf, inf, inf, inf] ไ\n",
      "[None, inf, inf, inf, inf, inf, inf, inf, inf] ป\n",
      "[None, None, 2, 2, 2, inf, inf, inf, inf] ห\n",
      "[None, None, None, inf, inf, inf, inf, inf, inf] า\n",
      "[None, None, None, None, 2, inf, inf, inf, 2] ม\n",
      "[None, None, None, None, None, inf, 3, inf, inf] เ\n",
      "[None, None, None, None, None, None, inf, inf, inf] ห\n",
      "[None, None, None, None, None, None, None, 4, 4] ส\n",
      "[None, None, None, None, None, None, None, None, inf] ี\n"
     ]
    }
   ],
   "source": [
    "input_text = \"ไปหามเหสี\"\n",
    "out = maximal_matching(input_text)\n",
    "for i in range(len(out)):\n",
    "    print(out[i],input_text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected tokenized text\n",
    "ไปหา|มเหสี"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ไปหา|มเหสี\n"
     ]
    }
   ],
   "source": [
    "print_tokenized_text(out,input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
